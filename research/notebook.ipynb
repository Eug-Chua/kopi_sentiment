{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kopi_sentiment.scraper.reddit import RedditScraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = RedditScraper()\n",
    "posts = scraper.fetch_posts(limit=5)\n",
    "posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = RedditScraper()\n",
    "posts = scraper.fetch_posts_with_content(limit=3)\n",
    "posts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts[1].comments[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = RedditScraper()\n",
    "posts = scraper.fetch_posts_with_content(limit=2)\n",
    "\n",
    "for post in posts:\n",
    "    print(f\"Title: {post.title}\")\n",
    "    print(f\"Comments ({len(post.comments)}):\")\n",
    "    for comment in post.comments[:3]:  # Show first 3 comments\n",
    "        print(f\"  - {comment[:100]}...\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kopi_sentiment.scraper.reddit import RedditScraper\n",
    "\n",
    "scraper = RedditScraper()\n",
    "posts = scraper.fetch_posts_with_content(limit=2)\n",
    "\n",
    "for post in posts:\n",
    "    print(f\"Title: {post.title}\")\n",
    "    print(f\"Comments ({len(post.comments)}):\")\n",
    "    for comment in post.comments[:5]:\n",
    "        print(f\"  [+{comment.score}] {comment.text[:80]}...\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kopi_sentiment.scraper.reddit import RedditScraper\n",
    "from kopi_sentiment.analyzer.claude import ClaudeAnalyzer\n",
    "\n",
    "# Fetch a post with comments\n",
    "scraper = RedditScraper()\n",
    "posts = scraper.fetch_posts_with_content(limit=1)\n",
    "post = posts[0]\n",
    "\n",
    "print(f\"Analyzing: {post.title}\")\n",
    "print(f\"Comments: {len(post.comments)}\")\n",
    "print(\"---\")\n",
    "\n",
    "# Analyze with Claude\n",
    "analyzer = ClaudeAnalyzer()\n",
    "result = analyzer.analyze(post)\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nüìä FFGA Analysis for: {result.post_title}\\n\")\n",
    "\n",
    "for category in [\"fears\", \"frustrations\", \"goals\", \"aspirations\"]:\n",
    "    ffga = getattr(result, category)\n",
    "    print(f\"**{category.upper()}** [{ffga.sentiment.value}]\")\n",
    "    print(f\"  Summary: {ffga.summary}\")\n",
    "    print(f\"  Quotes: {ffga.quotes[:2]}\")  # First 2 quotes\n",
    "    print()\n",
    "\n",
    "print(f\"üéØ Overall Sentiment: {result.overall_sentiment.value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kopi_sentiment.scraper.reddit import RedditScraper\n",
    "from kopi_sentiment.analyzer.openai import OpenAIAnalyzer\n",
    "\n",
    "# Fetch a post with comments\n",
    "scraper = RedditScraper()\n",
    "posts = scraper.fetch_posts_with_content(limit=1)\n",
    "post = posts[0]\n",
    "\n",
    "print(f\"Analyzing: {post.title}\")\n",
    "print(f\"Comments: {len(post.comments)}\")\n",
    "print(\"---\")\n",
    "\n",
    "# Analyze with Claude\n",
    "analyzer = OpenAIAnalyzer()\n",
    "result = analyzer.analyze(post)\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nüìä FFGA Analysis for: {result.post_title}\\n\")\n",
    "\n",
    "for category in [\"fears\", \"frustrations\", \"goals\", \"aspirations\"]:\n",
    "    ffga = getattr(result, category)\n",
    "    print(f\"**{category.upper()}** [{ffga.sentiment.value}]\")\n",
    "    print(f\"  Summary: {ffga.summary}\")\n",
    "    print(f\"  Quotes: {ffga.quotes[:2]}\")  # First 2 quotes\n",
    "    print()\n",
    "\n",
    "print(f\"üéØ Overall Sentiment: {result.overall_sentiment.value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kopi_sentiment.scraper.reddit import RedditScraper\n",
    "from kopi_sentiment.analyzer.openai import OpenAIAnalyzer\n",
    "\n",
    "# Fetch a post with comments\n",
    "scraper = RedditScraper()\n",
    "posts = scraper.fetch_posts_with_content(limit=9)\n",
    "post = posts[-1]\n",
    "\n",
    "print(f\"Analyzing: {post.title}\")\n",
    "print(f\"Comments: {len(post.comments)}\")\n",
    "print(\"---\")\n",
    "\n",
    "# Analyze\n",
    "analyzer = OpenAIAnalyzer()\n",
    "result = analyzer.analyze(post)\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nüìä FFGA Analysis for: {result.post_title}\\n\")\n",
    "\n",
    "for category in [\"fears\", \"frustrations\", \"goals\", \"aspirations\"]:\n",
    "    ffga = getattr(result, category)\n",
    "    print(f\"**{category.upper()}** [{ffga.sentiment.value}]\")\n",
    "    print(f\"  Summary: {ffga.summary}\")\n",
    "    print(f\"  Quotes: {ffga.quotes[:5]}\")  # First 2 quotes\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kopi_sentiment.scraper.reddit import RedditScraper\n",
    "from kopi_sentiment.analyzer.claude import ClaudeAnalyzer\n",
    "\n",
    "# Fetch a post with comments\n",
    "scraper = RedditScraper()\n",
    "posts = scraper.fetch_posts_with_content(limit=9)\n",
    "post = posts[-1]\n",
    "\n",
    "print(f\"Analyzing: {post.title}\")\n",
    "print(f\"Comments: {len(post.comments)}\")\n",
    "print(\"---\")\n",
    "\n",
    "# Analyze\n",
    "analyzer = ClaudeAnalyzer()\n",
    "result = analyzer.analyze(post)\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nüìä FFGA Analysis for: {result.post_title}\\n\")\n",
    "\n",
    "for category in [\"fears\", \"frustrations\", \"goals\", \"aspirations\"]:\n",
    "    ffga = getattr(result, category)\n",
    "    print(f\"**{category.upper()}** [{ffga.sentiment.value}]\")\n",
    "    print(f\"  Summary: {ffga.summary}\")\n",
    "    print(f\"  Quotes: {ffga.quotes[:5]}\")  # First 5 quotes\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kopi_sentiment.scraper.reddit import RedditScraper\n",
    "\n",
    "scraper = RedditScraper()\n",
    "\n",
    "# Test search for \"layoffs\"\n",
    "posts = scraper.search_posts_with_content(\"hdb\", limit=5)\n",
    "\n",
    "for post in posts:\n",
    "    print(f\"Title: {post.title}\")\n",
    "    print(f\"Score: {post.score} | Comments: {len(post.comments)}\")\n",
    "    print(f\"Top comment: {post.comments[0].text[:100] if post.comments else 'None'}...\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kopi_sentiment.scraper.reddit import RedditScraper\n",
    "\n",
    "scraper = RedditScraper()\n",
    "\n",
    "# Step 1: Test just the search (no content fetching)\n",
    "posts = scraper.search_posts(\"hdb\", limit=5)\n",
    "print(f\"Found {len(posts)} posts\")\n",
    "\n",
    "# Step 2: If posts found, print them\n",
    "for post in posts:\n",
    "    print(f\"- {post.title}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://old.reddit.com/r/singapore/search\"\n",
    "params = {\n",
    "    \"q\": \"hdb\",\n",
    "    \"restrict_sr\": \"on\",\n",
    "    \"sort\": \"comments\",\n",
    "    \"t\": \"month\",\n",
    "}\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\"\n",
    "})\n",
    "\n",
    "response = session.get(url, params=params)\n",
    "print(f\"Status: {response.status_code}\")\n",
    "print(f\"URL: {response.url}\")\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "things = soup.find_all(\"div\", class_=\"thing\")\n",
    "print(f\"Found {len(things)} 'thing' divs\")\n",
    "\n",
    "# Check if there's a search-result class instead\n",
    "search_results = soup.find_all(\"div\", class_=\"search-result\")\n",
    "print(f\"Found {len(search_results)} 'search-result' divs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Clementi sees a new record with its first S$1.5M HDB resale\n",
      "Score: 271 | Comments: 25\n",
      "Top comment: 94 and a half years left on the lease, someone earnt alot off their BTO... Remember when we were com...\n",
      "---\n",
      "Title: Over 13,400 HDB flats to reach MOP in 2026; analysts say supply could moderate resale price growth\n",
      "Score: 106 | Comments: 25\n",
      "Top comment: Calling it we‚Äôre going to see the first $2 mil HDB flat in 2026 Edit: additional. The monitoring wil...\n",
      "---\n",
      "Title: Hougang maisonette resale flat with around 66 years left on lease sells for record S$1.45 million\n",
      "Score: 200 | Comments: 24\n",
      "Top comment: At this rate, the monitoring will only stop and action taken when a Woodlands flat sells for 1+ mill...\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from kopi_sentiment.scraper.reddit import RedditScraper\n",
    "\n",
    "scraper = RedditScraper()\n",
    "\n",
    "# Test search for \"hdb\" posts from past month, sorted by comments\n",
    "posts = scraper.search_posts_with_content(\"hdb\", limit=3)\n",
    "\n",
    "for post in posts:\n",
    "    print(f\"Title: {post.title}\")\n",
    "    print(f\"Score: {post.score} | Comments: {len(post.comments)}\")\n",
    "    if post.comments:\n",
    "        print(f\"Top comment: {post.comments[0].text[:100]}...\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 posts about HDB\n",
      "---\n",
      "\n",
      "üìù Analyzing: Clementi sees a new record with its first S$1.5M HDB resale\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to parse extraction response: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**FEARS** [mixed]\n",
      "  No relevant comments found.\n",
      "\n",
      "**FRUSTRATIONS** [mixed]\n",
      "  No relevant comments found.\n",
      "\n",
      "**GOALS** [mixed]\n",
      "  No relevant comments found.\n",
      "\n",
      "**ASPIRATIONS** [mixed]\n",
      "  No relevant comments found.\n",
      "\n",
      "==================================================\n",
      "\n",
      "üìù Analyzing: Over 13,400 HDB flats to reach MOP in 2026; analysts say supply could moderate resale price growth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to parse extraction response: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**FEARS** [mixed]\n",
      "  No relevant comments found.\n",
      "\n",
      "**FRUSTRATIONS** [mixed]\n",
      "  No relevant comments found.\n",
      "\n",
      "**GOALS** [mixed]\n",
      "  No relevant comments found.\n",
      "\n",
      "**ASPIRATIONS** [mixed]\n",
      "  No relevant comments found.\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from kopi_sentiment.scraper.reddit import RedditScraper\n",
    "# from kopi_sentiment.analyzer.claude import ClaudeAnalyzer\n",
    "from kopi_sentiment.analyzer.openai import OpenAIAnalyzer\n",
    "\n",
    "# 1. Search for HDB posts\n",
    "scraper = RedditScraper()\n",
    "posts = scraper.search_posts_with_content(\"hdb\", limit=2)\n",
    "\n",
    "print(f\"Found {len(posts)} posts about HDB\")\n",
    "print(\"---\")\n",
    "\n",
    "# 2. Analyze each post\n",
    "analyzer = OpenAIAnalyzer()\n",
    "\n",
    "for post in posts:\n",
    "    print(f\"\\nüìù Analyzing: {post.title}\\n\")\n",
    "    \n",
    "    result = analyzer.analyze(post)\n",
    "    \n",
    "    for category in [\"fears\", \"frustrations\", \"goals\", \"aspirations\"]:\n",
    "        ffga = getattr(result, category)\n",
    "        print(f\"**{category.upper()}** [{ffga.sentiment.value}]\")\n",
    "        print(f\"  {ffga.summary}\")\n",
    "        if ffga.quotes:\n",
    "            print(f\"  Quote: \\\"{ffga.quotes[0][:80]}...\\\"\")\n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Clementi sees a new record with its first S$1.5M HDB resale\n",
      "Selftext: Welcome to/r/singapore! We're a community of locals, expats, tourists, and everyone else who's inter\n",
      "Number of comments: 25\n",
      "Subreddit: singapore\n",
      "\n",
      "First 3 comments:\n",
      "  [310] 94 and a half years left on the lease, someone earnt alot off their BTO... Remem...\n",
      "  [124] Inflation and the concept of compounding is easy enough to understand. What peop...\n",
      "  [81] But have incomes kept pace? At Median and at the \"market clearing\" rate of less ...\n"
     ]
    }
   ],
   "source": [
    "from kopi_sentiment.scraper.reddit import RedditScraper\n",
    "\n",
    "scraper = RedditScraper()\n",
    "posts = scraper.search_posts_with_content(\"hdb\", limit=1)\n",
    "\n",
    "post = posts[0]\n",
    "print(f\"Title: {post.title}\")\n",
    "print(f\"Selftext: {post.selftext[:100] if post.selftext else '(empty)'}\")\n",
    "print(f\"Number of comments: {len(post.comments)}\")\n",
    "print(f\"Subreddit: {post.subreddit}\")\n",
    "\n",
    "if post.comments:\n",
    "    print(\"\\nFirst 3 comments:\")\n",
    "    for c in post.comments[:3]:\n",
    "        print(f\"  [{c.score}] {c.text[:80]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== USER PROMPT ===\n",
      "Analyze the following Reddit post and comments from r/singapore.\n",
      "\n",
      "**Post Title**: Clementi sees a new record with its first S$1.5M HDB resale\n",
      "\n",
      "**Post Content**: Welcome to/r/singapore! We're a community of locals, expats, tourists, and everyone else who's interested in this Little Red Dot in one way or another.New to Reddit?Read this first.Got a question?/r/AskSingaporeRulesPlease observereddiquette. Offenders will be banned with no warning.Disallowed submissions: survey requests, spam, racism, sexism, unlabelled NSFW content, personal attacks, personal information (doxxing), job postings, buying and selling.Submissions must have titles comprised of the exact copied and pasted headline from an article. Do not add, remove or change words.Do not use cached or archived links for submissions.Keep all small questions (\"Where to buy X\") and submissions that are unrelated to Singapore (\"Looking for people to do X\") to the daily thread.Off topic and low effort submissions (memes) will be remov\n",
      "...\n",
      "\n",
      "=== LLM RESPONSE ===\n",
      "Response type: <class 'str'>\n",
      "Response: ''\n"
     ]
    }
   ],
   "source": [
    "from kopi_sentiment.analyzer.openai import OpenAIAnalyzer\n",
    "from kopi_sentiment.analyzer.prompts import EXTRACT_SYSTEM_PROMPT, build_extract_prompt\n",
    "from kopi_sentiment.scraper.reddit import RedditScraper\n",
    "\n",
    "# Get a post\n",
    "scraper = RedditScraper()\n",
    "posts = scraper.search_posts_with_content(\"hdb\", limit=1)\n",
    "post = posts[0]\n",
    "\n",
    "# Build the prompt\n",
    "user_prompt = build_extract_prompt(\n",
    "    title=post.title,\n",
    "    selftext=post.selftext,\n",
    "    comments=post.comments,\n",
    "    subreddit=post.subreddit\n",
    ")\n",
    "\n",
    "print(\"=== USER PROMPT ===\")\n",
    "print(user_prompt[:1000])\n",
    "print(\"...\")\n",
    "\n",
    "# Make direct LLM call\n",
    "analyzer = OpenAIAnalyzer()\n",
    "response = analyzer._call_llm(EXTRACT_SYSTEM_PROMPT, user_prompt)\n",
    "\n",
    "print(\"\\n=== LLM RESPONSE ===\")\n",
    "print(f\"Response type: {type(response)}\")\n",
    "print(f\"Response: '{response}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXPANDO FOUND ===\n",
      "No expando found\n",
      "\n",
      "=== DATA-URL ===\n",
      "https://www.99.co/singapore/insider/clementi-s1-5m-hdb-resale/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://old.reddit.com/r/singapore/comments/1q53g6o/clementi_sees_a_new_record_with_its_first_s15m/\"\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\"\n",
    "})\n",
    "\n",
    "response = session.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find the main post area\n",
    "siteTable = soup.find(\"div\", id=\"siteTable\")\n",
    "if siteTable:\n",
    "    expando = siteTable.find(\"div\", class_=\"expando\")\n",
    "    print(\"=== EXPANDO FOUND ===\")\n",
    "    if expando:\n",
    "        usertext = expando.find(\"div\", class_=\"usertext-body\")\n",
    "        if usertext:\n",
    "            print(usertext.get_text(strip=True)[:500])\n",
    "        else:\n",
    "            print(\"No usertext-body in expando\")\n",
    "    else:\n",
    "        print(\"No expando found\")\n",
    "else:\n",
    "    print(\"No siteTable found\")\n",
    "\n",
    "# Also check - is this a link post?\n",
    "thing = soup.find(\"div\", class_=\"thing\")\n",
    "if thing:\n",
    "    data_url = thing.get(\"data-url\", \"\")\n",
    "    print(f\"\\n=== DATA-URL ===\\n{data_url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: '```json\n",
      "{\n",
      "    \"fears\": [\n",
      "        \"At the same rate, we'll hit $2m in 2029. That's just 4 years.\",\n",
      "        \"For those who have not yet bought housing yet and fixed in a mortgage, there is a sense then of \\\"running to stay in place\\\" or a \\\"fear of being left behind\\\".\",\n",
      "        \"It's only going to get worse.\",\n",
      "        \"I'm demonstrating that the most expensive public housing is getting further and further away from each generation.\"\n",
      "    ],\n",
      "    \"frustrations\": [\n",
      "        \"But have incomes kept pace'\n"
     ]
    }
   ],
   "source": [
    "from kopi_sentiment.analyzer.claude import ClaudeAnalyzer\n",
    "from kopi_sentiment.analyzer.prompts import EXTRACT_SYSTEM_PROMPT, build_extract_prompt\n",
    "from kopi_sentiment.scraper.reddit import RedditScraper\n",
    "\n",
    "scraper = RedditScraper()\n",
    "posts = scraper.search_posts_with_content(\"hdb\", limit=1)\n",
    "post = posts[0]\n",
    "\n",
    "# Try with Claude\n",
    "analyzer = ClaudeAnalyzer()\n",
    "user_prompt = build_extract_prompt(\n",
    "    title=post.title,\n",
    "    selftext=\"\",  # Empty selftext for link post\n",
    "    comments=post.comments,\n",
    "    subreddit=post.subreddit\n",
    ")\n",
    "\n",
    "response = analyzer._call_llm(EXTRACT_SYSTEM_PROMPT, user_prompt)\n",
    "print(f\"Response: '{response[:500]}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n    \"fears\": [\\n        \"At the same rate, we\\'ll hit $2m in 2029. That\\'s just 4 years.\",\\n        \"For those who have not yet bought housing yet and fixed in a mortgage, there is a sense then of \\\\\"running to stay in place\\\\\" or a \\\\\"fear of being left behind\\\\\".\",\\n        \"It\\'s only going to get worse.\",\\n        \"I\\'m demonstrating that the most expensive public housing is getting further and further away from each generation.\"\\n    ],\\n    \"frustrations\": [\\n        \"But have incomes kept pace? At Median and at the \\\\\"market clearing\\\\\" rate of less skilled labour\",\\n        \"57% increase over the last 4 years is huge and I doubt most people receive 57% increase in income over the last 4 years.\",\\n        \"We shouldnt even be following suit in the first place since the our govt has control over public housing. Other govts do not have the same setup. Unfortunately the current govt rarely takes firm action. They\\'ve been monitoring this for yearsss.\",\\n        \"from monitoring to almost total silent‚Ä¶ is it me or chee hong tat is remarkably much more quiet about the housing situation than that monitor lizard.\",\\n        \"Which is an annoying for democracy because owners tend to outnumber buyers.\"\\n    ],\\n    \"goals\": [\\n        \"why dont i just quit my job and go for a bto with max subsidies\",\\n        \"With any luck we can have a suite of policies to replicate that price plateau again\",\\n        \"I think in terms of looking at affordability it should really be tracking BTO prices, not resell prices.\"\\n    ],\\n    \"aspirations\": [\\n        \"Who needs toto when you have BTO. Their profit is probably 800k\"\\n    ]\\n}\\n```'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with: Clementi sees a new record with its first S$1.5M HDB resale\n",
      "\n",
      "==================================================\n",
      "\n",
      "üîµ CLAUDE:\n",
      "  FEARS: [strong_negative] Deep anxiety about being priced out of housing market with c...\n",
      "  FRUSTRATIONS: [strong_negative] Intense frustration over income stagnation failing to match ...\n",
      "  GOALS: [mixed] Mixed views ranging from cynical profit-seeking through BTO ...\n",
      "  ASPIRATIONS: [positive] Hopeful desire for policy interventions that could stabilize...\n",
      "\n",
      "==================================================\n",
      "\n",
      "üü¢ OPENAI:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to parse extraction response: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  FEARS: [mixed] No relevant comments found....\n",
      "  FRUSTRATIONS: [mixed] No relevant comments found....\n",
      "  GOALS: [mixed] No relevant comments found....\n",
      "  ASPIRATIONS: [mixed] No relevant comments found....\n"
     ]
    }
   ],
   "source": [
    "from kopi_sentiment.scraper.reddit import RedditScraper\n",
    "from kopi_sentiment.analyzer.claude import ClaudeAnalyzer\n",
    "from kopi_sentiment.analyzer.openai import OpenAIAnalyzer\n",
    "\n",
    "# Get a post\n",
    "scraper = RedditScraper()\n",
    "posts = scraper.search_posts_with_content(\"hdb\", limit=1)\n",
    "post = posts[0]\n",
    "\n",
    "print(f\"Testing with: {post.title}\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test Claude\n",
    "print(\"\\nüîµ CLAUDE:\")\n",
    "claude = ClaudeAnalyzer()\n",
    "result = claude.analyze(post)\n",
    "for cat in [\"fears\", \"frustrations\", \"goals\", \"aspirations\"]:\n",
    "    ffga = getattr(result, cat)\n",
    "    print(f\"  {cat.upper()}: [{ffga.sentiment.value}] {ffga.summary[:60]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# Test OpenAI\n",
    "print(\"\\nüü¢ OPENAI:\")\n",
    "openai = OpenAIAnalyzer()\n",
    "result = openai.analyze(post)\n",
    "for cat in [\"fears\", \"frustrations\", \"goals\", \"aspirations\"]:\n",
    "    ffga = getattr(result, cat)\n",
    "    print(f\"  {cat.upper()}: [{ffga.sentiment.value}] {ffga.summary[:60]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling OpenAI...\n",
      "Response type: <class 'str'>\n",
      "Response length: 0\n",
      "Response repr: ''\n",
      "Response content: EMPTY\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from kopi_sentiment.analyzer.openai import OpenAIAnalyzer\n",
    "from kopi_sentiment.analyzer.prompts import EXTRACT_SYSTEM_PROMPT, build_extract_prompt\n",
    "\n",
    "# Create analyzer\n",
    "openai_analyzer = OpenAIAnalyzer()\n",
    "\n",
    "# Build the same prompt we'd use for extraction\n",
    "user_prompt = build_extract_prompt(\n",
    "    title=post.title,\n",
    "    selftext=post.selftext,\n",
    "    comments=post.comments,\n",
    "    subreddit=post.subreddit,\n",
    ")\n",
    "\n",
    "# Test the raw LLM call\n",
    "print(\"Calling OpenAI...\")\n",
    "raw_response = openai_analyzer._call_llm(EXTRACT_SYSTEM_PROMPT, user_prompt)\n",
    "print(f\"Response type: {type(raw_response)}\")\n",
    "print(f\"Response length: {len(raw_response)}\")\n",
    "print(f\"Response repr: {repr(raw_response)}\")\n",
    "print(f\"Response content: {raw_response[:500] if raw_response else 'EMPTY'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: gpt-5-nano\n",
      "Finish reason: length\n",
      "Content: \n",
      "Usage: CompletionUsage(completion_tokens=100, prompt_tokens=32, total_tokens=132, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=100, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from kopi_sentiment.config.settings import settings\n",
    "\n",
    "client = OpenAI(api_key=settings.openai_api_key)\n",
    "\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=settings.openai_model,\n",
    "        max_completion_tokens=100,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Respond in JSON.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Say hello in JSON format: {\\\"greeting\\\": \\\"...\\\"}\"},\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "    print(f\"Model: {settings.openai_model}\")\n",
    "    print(f\"Finish reason: {response.choices[0].finish_reason}\")\n",
    "    print(f\"Content: {response.choices[0].message.content}\")\n",
    "    print(f\"Usage: {response.usage}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {type(e).__name__}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with: Clementi sees a new record with its first S$1.5M HDB resale\n",
      "\n",
      "\n",
      "üîµ CLAUDE:\n",
      "  FEARS: [strong_negative] Deep anxiety about housing prices spiraling out of control, ...\n",
      "  FRUSTRATIONS: [strong_negative] Intense anger over the disconnect between stagnant wages and...\n",
      "  GOALS: [mixed] Pragmatic focus on BTO as the viable path to homeownership, ...\n",
      "  ASPIRATIONS: [positive] Hopeful desire for policy interventions that could stabilize...\n",
      "\n",
      "üü¢ OPENAI:\n",
      "  FEARS: [negative] There is a palpable sense of anxiety regarding job stability...\n",
      "  FRUSTRATIONS: [strong_negative] Participants express significant frustration with the rapid ...\n",
      "  GOALS: [positive] Despite challenges, there is a hope that maintaining a stabl...\n",
      "  ASPIRATIONS: [positive] There is optimism for policy measures that could stabilize h...\n"
     ]
    }
   ],
   "source": [
    "from kopi_sentiment.scraper.reddit import RedditScraper\n",
    "from kopi_sentiment.analyzer.claude import ClaudeAnalyzer\n",
    "from kopi_sentiment.analyzer.openai import OpenAIAnalyzer\n",
    "\n",
    "# Get a post\n",
    "scraper = RedditScraper()\n",
    "posts = scraper.search_posts_with_content(\"hdb\", limit=1)\n",
    "post = posts[0]\n",
    "\n",
    "print(f\"Testing with: {post.title}\\n\")\n",
    "\n",
    "# Test both\n",
    "for name, analyzer in [(\"CLAUDE\", ClaudeAnalyzer()), (\"OPENAI\", OpenAIAnalyzer())]:\n",
    "    print(f\"\\n{'üîµ' if name == 'CLAUDE' else 'üü¢'} {name}:\")\n",
    "    result = analyzer.analyze(post)\n",
    "    for cat in [\"fears\", \"frustrations\", \"goals\", \"aspirations\"]:\n",
    "        ffga = getattr(result, cat)\n",
    "        print(f\"  {cat.upper()}: [{ffga.sentiment.value}] {ffga.summary[:60]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for: HDB\n",
      "  Found 5 posts, total comments: 125\n",
      "Searching for: BTO\n",
      "  Found 2 posts, total comments: 175\n",
      "Searching for: housing prices\n",
      "  Found 1 posts, total comments: 200\n",
      "Searching for: cost of living\n",
      "  Found 1 posts, total comments: 225\n",
      "Searching for: CPF\n",
      "  Found 3 posts, total comments: 299\n",
      "Searching for: salary\n",
      "  Found 3 posts, total comments: 374\n",
      "Searching for: work life balance\n",
      "  Found 5 posts, total comments: 495\n",
      "Searching for: inflation\n",
      "  Found 2 posts, total comments: 539\n",
      "\n",
      "Total comments collected: 539\n",
      "Saved to data/comments_raw.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "from kopi_sentiment.scraper.reddit import RedditScraper\n",
    "\n",
    "# Topics to search for diverse FFGA content\n",
    "topics = [\n",
    "    \"HDB\",\n",
    "    \"BTO\", \n",
    "    \"housing prices\",\n",
    "    \"cost of living\",\n",
    "    \"CPF\",\n",
    "    \"salary\",\n",
    "    \"work life balance\",\n",
    "    \"inflation\",\n",
    "]\n",
    "\n",
    "scraper = RedditScraper()\n",
    "all_comments = []\n",
    "\n",
    "for topic in topics:\n",
    "    print(f\"Searching for: {topic}\")\n",
    "    posts = scraper.search_posts_with_content(topic, limit=5, delay=1.0)\n",
    "    \n",
    "    for post in posts:\n",
    "        for comment in post.comments:\n",
    "            all_comments.append({\n",
    "                \"text\": comment.text,\n",
    "                \"score\": comment.score,\n",
    "                \"post_title\": post.title,\n",
    "                \"post_id\": post.id,\n",
    "                \"topic\": topic,\n",
    "            })\n",
    "    \n",
    "    print(f\"  Found {len(posts)} posts, total comments: {len(all_comments)}\")\n",
    "    time.sleep(2)  # Rate limiting between topics\n",
    "\n",
    "print(f\"\\nTotal comments collected: {len(all_comments)}\")\n",
    "\n",
    "# Save to CSV\n",
    "# Get project root (parent of research/)\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "data_path = os.path.join(project_root, \"data\", \"comments_raw.csv\")\n",
    "\n",
    "# Ensure data folder exists\n",
    "os.makedirs(os.path.dirname(data_path), exist_ok=True)\n",
    "\n",
    "with open(data_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"text\", \"score\", \"post_title\", \"post_id\", \"topic\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_comments)\n",
    "\n",
    "print(\"Saved to data/comments_raw.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comments: 539\n",
      "\n",
      "Comments per topic:\n",
      "topic\n",
      "HDB                  125\n",
      "work life balance    121\n",
      "salary                75\n",
      "CPF                   74\n",
      "BTO                   50\n",
      "inflation             44\n",
      "housing prices        25\n",
      "cost of living        25\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/comments_raw.csv\")\n",
    "print(f\"Total comments: {len(df)}\")\n",
    "print(f\"\\nComments per topic:\")\n",
    "print(df['topic'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled 50/539 comments...\n",
      "Labeled 100/539 comments...\n",
      "Labeled 150/539 comments...\n",
      "Labeled 200/539 comments...\n",
      "Labeled 250/539 comments...\n",
      "Labeled 300/539 comments...\n",
      "Labeled 350/539 comments...\n",
      "Labeled 400/539 comments...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from anthropic import Anthropic\n",
    "from kopi_sentiment.config.settings import settings\n",
    "\n",
    "# Load comments\n",
    "df = pd.read_csv(\"../data/comments_raw.csv\")\n",
    "\n",
    "# Initialize Claude\n",
    "client = Anthropic(api_key=settings.anthropic_api_key)\n",
    "\n",
    "LABELING_PROMPT = \"\"\"Classify this Reddit comment into exactly ONE category:\n",
    "\n",
    "- fear: Worries, anxieties about the FUTURE or uncertainty\n",
    "- frustration: Current complaints, things not working NOW  \n",
    "- goal: What the person is actively trying to achieve (actionable, specific)\n",
    "- aspiration: Ideal future states, hopes, dreams (idealistic, general)\n",
    "- none: Doesn't fit any category (jokes, questions, neutral info)\n",
    "\n",
    "Comment: \"{comment}\"\n",
    "\n",
    "Respond with ONLY the category name (fear/frustration/goal/aspiration/none), nothing else.\"\"\"\n",
    "\n",
    "def label_comment(comment: str) -> str:\n",
    "    \"\"\"Label a single comment using Claude.\"\"\"\n",
    "    try:\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-sonnet-4-20250514\",\n",
    "            max_tokens=10,\n",
    "            messages=[{\"role\": \"user\", \"content\": LABELING_PROMPT.format(comment=comment)}]\n",
    "        )\n",
    "        return response.content[0].text.strip().lower()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"error\"\n",
    "\n",
    "# Label all comments\n",
    "labels = []\n",
    "for i, row in df.iterrows():\n",
    "    label = label_comment(row['text'])\n",
    "    labels.append(label)\n",
    "    \n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"Labeled {i + 1}/{len(df)} comments...\")\n",
    "    \n",
    "    time.sleep(0.1)  # Rate limiting\n",
    "\n",
    "df['label'] = labels\n",
    "\n",
    "# Save labeled data\n",
    "df.to_csv(\"../data/comments_labeled.csv\", index=False)\n",
    "\n",
    "print(f\"\\nLabeling complete!\")\n",
    "print(df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from anthropic import Anthropic\n",
    "from kopi_sentiment.config.settings import settings\n",
    "\n",
    "df = pd.read_csv(\"../data/comments_raw.csv\")\n",
    "client = Anthropic(api_key=settings.anthropic_api_key)\n",
    "\n",
    "LABELING_PROMPT = \"\"\"Classify this Reddit comment into exactly ONE category:\n",
    "\n",
    "- fear: Worries, anxieties about the FUTURE or uncertainty\n",
    "- frustration: Current complaints, things not working NOW  \n",
    "- goal: What the person is actively trying to achieve (actionable, specific)\n",
    "- aspiration: Ideal future states, hopes, dreams (idealistic, general)\n",
    "- none: Doesn't fit any category (jokes, questions, neutral info)\n",
    "\n",
    "Comment: \"{comment}\"\n",
    "\n",
    "Respond with ONLY the category name (fear/frustration/goal/aspiration/none), nothing else.\"\"\"\n",
    "\n",
    "def label_comment(comment: str) -> str:\n",
    "    try:\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-sonnet-4-20250514\",\n",
    "            max_tokens=10,\n",
    "            messages=[{\"role\": \"user\", \"content\": LABELING_PROMPT.format(comment=comment)}]\n",
    "        )\n",
    "        return response.content[0].text.strip().lower()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"error\"\n",
    "\n",
    "# Add label column if not exists\n",
    "if 'label' not in df.columns:\n",
    "    df['label'] = None\n",
    "\n",
    "# Process only unlabeled rows\n",
    "for i, row in df.iterrows():\n",
    "    if pd.isna(row.get('label')) or row.get('label') is None:\n",
    "        label = label_comment(row['text'])\n",
    "        df.at[i, 'label'] = label\n",
    "        \n",
    "        # Save every 50 comments\n",
    "        if (i + 1) % 50 == 0:\n",
    "            df.to_csv(\"../data/comments_labeled.csv\", index=False)\n",
    "            print(f\"Saved checkpoint at {i + 1}/{len(df)}\")\n",
    "        \n",
    "        time.sleep(0.1)\n",
    "\n",
    "# Final save\n",
    "df.to_csv(\"../data/comments_labeled.csv\", index=False)\n",
    "print(f\"\\nDone! Label distribution:\")\n",
    "print(df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "source": "import json\nimport time\nimport pandas as pd\nfrom anthropic import Anthropic\nfrom kopi_sentiment.config.settings import settings\n\n# Load raw comments (start fresh for dual labeling)\ndf = pd.read_csv(\"../data/comments_raw.csv\")\nclient = Anthropic(api_key=settings.anthropic_api_key)\n\nBATCH_LABELING_PROMPT = \"\"\"Classify these Reddit comments. For EACH comment, provide:\n1. category: fear/frustration/goal/aspiration/none\n2. sentiment: strong_positive/positive/mixed/negative/strong_negative\n\nCategories:\n- fear: Worries, anxieties about FUTURE or uncertainty\n- frustration: Current complaints, things not working NOW  \n- goal: Actively trying to achieve something (actionable, specific)\n- aspiration: Ideal future states, hopes, dreams (idealistic)\n- none: Doesn't fit (jokes, questions, neutral info)\n\nSentiment intensity:\n- strong_positive: Enthusiastic, excited, very optimistic\n- positive: Hopeful, satisfied, mildly optimistic\n- mixed: Contains both positive and negative elements\n- negative: Disappointed, unhappy, pessimistic\n- strong_negative: Angry, fearful, very pessimistic\n\nComments:\n{comments_block}\n\nRespond in JSON array format:\n[\n  {{\"id\": 0, \"category\": \"...\", \"sentiment\": \"...\"}},\n  {{\"id\": 1, \"category\": \"...\", \"sentiment\": \"...\"}}\n]\n\nONLY output the JSON array, nothing else.\"\"\"\n\ndef label_batch(comments: list[tuple[int, str]]) -> list[dict]:\n    \"\"\"Label a batch of comments, returns list of {id, category, sentiment}.\"\"\"\n    comments_block = \"\\n\".join([f\"[{i}] {text[:500]}\" for i, text in comments])\n    \n    try:\n        response = client.messages.create(\n            model=\"claude-sonnet-4-20250514\",\n            max_tokens=1024,\n            messages=[{\"role\": \"user\", \"content\": BATCH_LABELING_PROMPT.format(comments_block=comments_block)}]\n        )\n        raw = response.content[0].text.strip()\n        # Clean markdown if present\n        if raw.startswith(\"```\"):\n            raw = raw.split(\"```\")[1]\n            if raw.startswith(\"json\"):\n                raw = raw[4:]\n        return json.loads(raw)\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return [{\"id\": i, \"category\": \"error\", \"sentiment\": \"error\"} for i, _ in comments]\n\n# Initialize columns\ndf['category'] = None\ndf['sentiment'] = None\n\nBATCH_SIZE = 10\ntotal_batches = (len(df) + BATCH_SIZE - 1) // BATCH_SIZE\n\nfor batch_num in range(total_batches):\n    start_idx = batch_num * BATCH_SIZE\n    end_idx = min(start_idx + BATCH_SIZE, len(df))\n    \n    # Build batch: list of (local_id, text)\n    batch = [(i - start_idx, df.iloc[i]['text']) for i in range(start_idx, end_idx)]\n    \n    # Label batch\n    results = label_batch(batch)\n    \n    # Apply results\n    for result in results:\n        row_idx = start_idx + result['id']\n        if row_idx < len(df):\n            df.at[row_idx, 'category'] = result.get('category', 'error')\n            df.at[row_idx, 'sentiment'] = result.get('sentiment', 'error')\n    \n    # Progress & checkpoint\n    if (batch_num + 1) % 5 == 0:\n        df.to_csv(\"../data/comments_labeled.csv\", index=False)\n        print(f\"Batch {batch_num + 1}/{total_batches} done, saved checkpoint\")\n    \n    time.sleep(0.5)  # Rate limiting\n\n# Final save\ndf.to_csv(\"../data/comments_labeled.csv\", index=False)\nprint(f\"\\nLabeling complete!\")\nprint(f\"\\nCategory distribution:\\n{df['category'].value_counts()}\")\nprint(f\"\\nSentiment distribution:\\n{df['sentiment'].value_counts()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kopi_sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}