{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kopi_sentiment.scraper.reddit import RedditScraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = RedditScraper()\n",
    "posts = scraper.fetch_posts(limit=5)\n",
    "posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = RedditScraper()\n",
    "posts = scraper.fetch_posts_with_content(limit=3)\n",
    "posts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts[1].comments[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = RedditScraper()\n",
    "posts = scraper.fetch_posts_with_content(limit=2)\n",
    "\n",
    "for post in posts:\n",
    "    print(f\"Title: {post.title}\")\n",
    "    print(f\"Comments ({len(post.comments)}):\")\n",
    "    for comment in post.comments[:3]:  # Show first 3 comments\n",
    "        print(f\"  - {comment[:100]}...\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kopi_sentiment.scraper.reddit import RedditScraper\n",
    "\n",
    "scraper = RedditScraper()\n",
    "posts = scraper.fetch_posts_with_content(limit=2)\n",
    "\n",
    "for post in posts:\n",
    "    print(f\"Title: {post.title}\")\n",
    "    print(f\"Comments ({len(post.comments)}):\")\n",
    "    for comment in post.comments[:5]:\n",
    "        print(f\"  [+{comment.score}] {comment.text[:80]}...\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kopi_sentiment.scraper.reddit import RedditScraper\n",
    "from kopi_sentiment.analyzer.claude import ClaudeAnalyzer\n",
    "\n",
    "# Fetch a post with comments\n",
    "scraper = RedditScraper()\n",
    "posts = scraper.fetch_posts_with_content(limit=1)\n",
    "post = posts[0]\n",
    "\n",
    "print(f\"Analyzing: {post.title}\")\n",
    "print(f\"Comments: {len(post.comments)}\")\n",
    "print(\"---\")\n",
    "\n",
    "# Analyze with Claude\n",
    "analyzer = ClaudeAnalyzer()\n",
    "result = analyzer.analyze(post)\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nðŸ“Š FFO Analysis for: {result.post_title}\\n\")\n",
    "\n",
    "for category in [\"fears\", \"frustrations\", \"goals\", \"aspirations\"]:\n",
    "    ffo = getattr(result, category)\n",
    "    print(f\"**{category.upper()}** [{ffo.sentiment.value}]\")\n",
    "    print(f\"  Summary: {ffo.summary}\")\n",
    "    print(f\"  Quotes: {ffo.quotes[:2]}\")  # First 2 quotes\n",
    "    print()\n",
    "\n",
    "print(f\"ðŸŽ¯ Overall Sentiment: {result.overall_sentiment.value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kopi_sentiment.scraper.reddit import RedditScraper\n",
    "from kopi_sentiment.analyzer.openai import OpenAIAnalyzer\n",
    "\n",
    "# Fetch a post with comments\n",
    "scraper = RedditScraper()\n",
    "posts = scraper.fetch_posts_with_content(limit=1)\n",
    "post = posts[0]\n",
    "\n",
    "print(f\"Analyzing: {post.title}\")\n",
    "print(f\"Comments: {len(post.comments)}\")\n",
    "print(\"---\")\n",
    "\n",
    "# Analyze with Claude\n",
    "analyzer = OpenAIAnalyzer()\n",
    "result = analyzer.analyze(post)\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nðŸ“Š FFO Analysis for: {result.post_title}\\n\")\n",
    "\n",
    "for category in [\"fears\", \"frustrations\", \"goals\", \"aspirations\"]:\n",
    "    ffo = getattr(result, category)\n",
    "    print(f\"**{category.upper()}** [{ffo.sentiment.value}]\")\n",
    "    print(f\"  Summary: {ffo.summary}\")\n",
    "    print(f\"  Quotes: {ffo.quotes[:2]}\")  # First 2 quotes\n",
    "    print()\n",
    "\n",
    "print(f\"ðŸŽ¯ Overall Sentiment: {result.overall_sentiment.value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kopi_sentiment.scraper.reddit import RedditScraper\n",
    "from kopi_sentiment.analyzer.openai import OpenAIAnalyzer\n",
    "\n",
    "# Fetch a post with comments\n",
    "scraper = RedditScraper()\n",
    "posts = scraper.fetch_posts_with_content(limit=9)\n",
    "post = posts[-1]\n",
    "\n",
    "print(f\"Analyzing: {post.title}\")\n",
    "print(f\"Comments: {len(post.comments)}\")\n",
    "print(\"---\")\n",
    "\n",
    "# Analyze\n",
    "analyzer = OpenAIAnalyzer()\n",
    "result = analyzer.analyze(post)\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nðŸ“Š FFO Analysis for: {result.post_title}\\n\")\n",
    "\n",
    "for category in [\"fears\", \"frustrations\", \"goals\", \"aspirations\"]:\n",
    "    ffo = getattr(result, category)\n",
    "    print(f\"**{category.upper()}** [{ffo.sentiment.value}]\")\n",
    "    print(f\"  Summary: {ffo.summary}\")\n",
    "    print(f\"  Quotes: {ffo.quotes[:5]}\")  # First 2 quotes\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kopi_sentiment.scraper.reddit import RedditScraper\n",
    "from kopi_sentiment.analyzer.claude import ClaudeAnalyzer\n",
    "\n",
    "# Fetch a post with comments\n",
    "scraper = RedditScraper()\n",
    "posts = scraper.fetch_posts_with_content(limit=9)\n",
    "post = posts[-1]\n",
    "\n",
    "print(f\"Analyzing: {post.title}\")\n",
    "print(f\"Comments: {len(post.comments)}\")\n",
    "print(\"---\")\n",
    "\n",
    "# Analyze\n",
    "analyzer = ClaudeAnalyzer()\n",
    "result = analyzer.analyze(post)\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nðŸ“Š FFO Analysis for: {result.post_title}\\n\")\n",
    "\n",
    "for category in [\"fears\", \"frustrations\", \"goals\", \"aspirations\"]:\n",
    "    ffo = getattr(result, category)\n",
    "    print(f\"**{category.upper()}** [{ffo.sentiment.value}]\")\n",
    "    print(f\"  Summary: {ffo.summary}\")\n",
    "    print(f\"  Quotes: {ffo.quotes[:5]}\")  # First 5 quotes\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kopi_sentiment.scraper.reddit import RedditScraper\n",
    "\n",
    "scraper = RedditScraper()\n",
    "\n",
    "# Test search for \"layoffs\"\n",
    "posts = scraper.search_posts_with_content(\"hdb\", limit=5)\n",
    "\n",
    "for post in posts:\n",
    "    print(f\"Title: {post.title}\")\n",
    "    print(f\"Score: {post.score} | Comments: {len(post.comments)}\")\n",
    "    print(f\"Top comment: {post.comments[0].text[:100] if post.comments else 'None'}...\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kopi_sentiment.scraper.reddit import RedditScraper\n",
    "\n",
    "scraper = RedditScraper()\n",
    "\n",
    "# Step 1: Test just the search (no content fetching)\n",
    "posts = scraper.search_posts(\"hdb\", limit=5)\n",
    "print(f\"Found {len(posts)} posts\")\n",
    "\n",
    "# Step 2: If posts found, print them\n",
    "for post in posts:\n",
    "    print(f\"- {post.title}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://old.reddit.com/r/singapore/search\"\n",
    "params = {\n",
    "    \"q\": \"hdb\",\n",
    "    \"restrict_sr\": \"on\",\n",
    "    \"sort\": \"comments\",\n",
    "    \"t\": \"month\",\n",
    "}\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\"\n",
    "})\n",
    "\n",
    "response = session.get(url, params=params)\n",
    "print(f\"Status: {response.status_code}\")\n",
    "print(f\"URL: {response.url}\")\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "things = soup.find_all(\"div\", class_=\"thing\")\n",
    "print(f\"Found {len(things)} 'thing' divs\")\n",
    "\n",
    "# Check if there's a search-result class instead\n",
    "search_results = soup.find_all(\"div\", class_=\"search-result\")\n",
    "print(f\"Found {len(search_results)} 'search-result' divs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kopi_sentiment.scraper.reddit import RedditScraper\n",
    "\n",
    "scraper = RedditScraper()\n",
    "\n",
    "# Test search for \"hdb\" posts from past month, sorted by comments\n",
    "posts = scraper.search_posts_with_content(\"hdb\", limit=3)\n",
    "\n",
    "for post in posts:\n",
    "    print(f\"Title: {post.title}\")\n",
    "    print(f\"Score: {post.score} | Comments: {len(post.comments)}\")\n",
    "    if post.comments:\n",
    "        print(f\"Top comment: {post.comments[0].text[:100]}...\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kopi_sentiment.scraper.reddit import RedditScraper\n",
    "# from kopi_sentiment.analyzer.claude import ClaudeAnalyzer\n",
    "from kopi_sentiment.analyzer.openai import OpenAIAnalyzer\n",
    "\n",
    "# 1. Search for HDB posts\n",
    "scraper = RedditScraper()\n",
    "posts = scraper.search_posts_with_content(\"hdb\", limit=2)\n",
    "\n",
    "print(f\"Found {len(posts)} posts about HDB\")\n",
    "print(\"---\")\n",
    "\n",
    "# 2. Analyze each post\n",
    "analyzer = OpenAIAnalyzer()\n",
    "\n",
    "for post in posts:\n",
    "    print(f\"\\nðŸ“ Analyzing: {post.title}\\n\")\n",
    "    \n",
    "    result = analyzer.analyze(post)\n",
    "    \n",
    "    for category in [\"fears\", \"frustrations\", \"goals\", \"aspirations\"]:\n",
    "        ffo = getattr(result, category)\n",
    "        print(f\"**{category.upper()}** [{ffo.sentiment.value}]\")\n",
    "        print(f\"  {ffo.summary}\")\n",
    "        if ffo.quotes:\n",
    "            print(f\"  Quote: \\\"{ffo.quotes[0][:80]}...\\\"\")\n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kopi_sentiment.scraper.reddit import RedditScraper\n",
    "\n",
    "scraper = RedditScraper()\n",
    "posts = scraper.search_posts_with_content(\"hdb\", limit=1)\n",
    "\n",
    "post = posts[0]\n",
    "print(f\"Title: {post.title}\")\n",
    "print(f\"Selftext: {post.selftext[:100] if post.selftext else '(empty)'}\")\n",
    "print(f\"Number of comments: {len(post.comments)}\")\n",
    "print(f\"Subreddit: {post.subreddit}\")\n",
    "\n",
    "if post.comments:\n",
    "    print(\"\\nFirst 3 comments:\")\n",
    "    for c in post.comments[:3]:\n",
    "        print(f\"  [{c.score}] {c.text[:80]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kopi_sentiment.analyzer.openai import OpenAIAnalyzer\n",
    "from kopi_sentiment.analyzer.prompts import EXTRACT_SYSTEM_PROMPT, build_extract_prompt\n",
    "from kopi_sentiment.scraper.reddit import RedditScraper\n",
    "\n",
    "# Get a post\n",
    "scraper = RedditScraper()\n",
    "posts = scraper.search_posts_with_content(\"hdb\", limit=1)\n",
    "post = posts[0]\n",
    "\n",
    "# Build the prompt\n",
    "user_prompt = build_extract_prompt(\n",
    "    title=post.title,\n",
    "    selftext=post.selftext,\n",
    "    comments=post.comments,\n",
    "    subreddit=post.subreddit\n",
    ")\n",
    "\n",
    "print(\"=== USER PROMPT ===\")\n",
    "print(user_prompt[:1000])\n",
    "print(\"...\")\n",
    "\n",
    "# Make direct LLM call\n",
    "analyzer = OpenAIAnalyzer()\n",
    "response = analyzer._call_llm(EXTRACT_SYSTEM_PROMPT, user_prompt)\n",
    "\n",
    "print(\"\\n=== LLM RESPONSE ===\")\n",
    "print(f\"Response type: {type(response)}\")\n",
    "print(f\"Response: '{response}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://old.reddit.com/r/singapore/comments/1q53g6o/clementi_sees_a_new_record_with_its_first_s15m/\"\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\"\n",
    "})\n",
    "\n",
    "response = session.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find the main post area\n",
    "siteTable = soup.find(\"div\", id=\"siteTable\")\n",
    "if siteTable:\n",
    "    expando = siteTable.find(\"div\", class_=\"expando\")\n",
    "    print(\"=== EXPANDO FOUND ===\")\n",
    "    if expando:\n",
    "        usertext = expando.find(\"div\", class_=\"usertext-body\")\n",
    "        if usertext:\n",
    "            print(usertext.get_text(strip=True)[:500])\n",
    "        else:\n",
    "            print(\"No usertext-body in expando\")\n",
    "    else:\n",
    "        print(\"No expando found\")\n",
    "else:\n",
    "    print(\"No siteTable found\")\n",
    "\n",
    "# Also check - is this a link post?\n",
    "thing = soup.find(\"div\", class_=\"thing\")\n",
    "if thing:\n",
    "    data_url = thing.get(\"data-url\", \"\")\n",
    "    print(f\"\\n=== DATA-URL ===\\n{data_url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kopi_sentiment.analyzer.claude import ClaudeAnalyzer\n",
    "from kopi_sentiment.analyzer.prompts import EXTRACT_SYSTEM_PROMPT, build_extract_prompt\n",
    "from kopi_sentiment.scraper.reddit import RedditScraper\n",
    "\n",
    "scraper = RedditScraper()\n",
    "posts = scraper.search_posts_with_content(\"hdb\", limit=1)\n",
    "post = posts[0]\n",
    "\n",
    "# Try with Claude\n",
    "analyzer = ClaudeAnalyzer()\n",
    "user_prompt = build_extract_prompt(\n",
    "    title=post.title,\n",
    "    selftext=\"\",  # Empty selftext for link post\n",
    "    comments=post.comments,\n",
    "    subreddit=post.subreddit\n",
    ")\n",
    "\n",
    "response = analyzer._call_llm(EXTRACT_SYSTEM_PROMPT, user_prompt)\n",
    "print(f\"Response: '{response[:500]}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kopi_sentiment.scraper.reddit import RedditScraper\n",
    "from kopi_sentiment.analyzer.claude import ClaudeAnalyzer\n",
    "from kopi_sentiment.analyzer.openai import OpenAIAnalyzer\n",
    "\n",
    "# Get a post\n",
    "scraper = RedditScraper()\n",
    "posts = scraper.search_posts_with_content(\"hdb\", limit=1)\n",
    "post = posts[0]\n",
    "\n",
    "print(f\"Testing with: {post.title}\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test Claude\n",
    "print(\"\\nðŸ”µ CLAUDE:\")\n",
    "claude = ClaudeAnalyzer()\n",
    "result = claude.analyze(post)\n",
    "for cat in [\"fears\", \"frustrations\", \"goals\", \"aspirations\"]:\n",
    "    ffo = getattr(result, cat)\n",
    "    print(f\"  {cat.upper()}: [{ffo.sentiment.value}] {ffo.summary[:60]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# Test OpenAI\n",
    "print(\"\\nðŸŸ¢ OPENAI:\")\n",
    "openai = OpenAIAnalyzer()\n",
    "result = openai.analyze(post)\n",
    "for cat in [\"fears\", \"frustrations\", \"goals\", \"aspirations\"]:\n",
    "    ffo = getattr(result, cat)\n",
    "    print(f\"  {cat.upper()}: [{ffo.sentiment.value}] {ffo.summary[:60]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from kopi_sentiment.analyzer.openai import OpenAIAnalyzer\n",
    "from kopi_sentiment.analyzer.prompts import EXTRACT_SYSTEM_PROMPT, build_extract_prompt\n",
    "\n",
    "# Create analyzer\n",
    "openai_analyzer = OpenAIAnalyzer()\n",
    "\n",
    "# Build the same prompt we'd use for extraction\n",
    "user_prompt = build_extract_prompt(\n",
    "    title=post.title,\n",
    "    selftext=post.selftext,\n",
    "    comments=post.comments,\n",
    "    subreddit=post.subreddit,\n",
    ")\n",
    "\n",
    "# Test the raw LLM call\n",
    "print(\"Calling OpenAI...\")\n",
    "raw_response = openai_analyzer._call_llm(EXTRACT_SYSTEM_PROMPT, user_prompt)\n",
    "print(f\"Response type: {type(raw_response)}\")\n",
    "print(f\"Response length: {len(raw_response)}\")\n",
    "print(f\"Response repr: {repr(raw_response)}\")\n",
    "print(f\"Response content: {raw_response[:500] if raw_response else 'EMPTY'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from kopi_sentiment.config.settings import settings\n",
    "\n",
    "client = OpenAI(api_key=settings.openai_api_key)\n",
    "\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=settings.openai_model,\n",
    "        max_completion_tokens=100,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Respond in JSON.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Say hello in JSON format: {\\\"greeting\\\": \\\"...\\\"}\"},\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "    print(f\"Model: {settings.openai_model}\")\n",
    "    print(f\"Finish reason: {response.choices[0].finish_reason}\")\n",
    "    print(f\"Content: {response.choices[0].message.content}\")\n",
    "    print(f\"Usage: {response.usage}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {type(e).__name__}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kopi_sentiment.scraper.reddit import RedditScraper\n",
    "from kopi_sentiment.analyzer.claude import ClaudeAnalyzer\n",
    "from kopi_sentiment.analyzer.openai import OpenAIAnalyzer\n",
    "\n",
    "# Get a post\n",
    "scraper = RedditScraper()\n",
    "posts = scraper.search_posts_with_content(\"hdb\", limit=1)\n",
    "post = posts[0]\n",
    "\n",
    "print(f\"Testing with: {post.title}\\n\")\n",
    "\n",
    "# Test both\n",
    "for name, analyzer in [(\"CLAUDE\", ClaudeAnalyzer()), (\"OPENAI\", OpenAIAnalyzer())]:\n",
    "    print(f\"\\n{'ðŸ”µ' if name == 'CLAUDE' else 'ðŸŸ¢'} {name}:\")\n",
    "    result = analyzer.analyze(post)\n",
    "    for cat in [\"fears\", \"frustrations\", \"goals\", \"aspirations\"]:\n",
    "        ffo = getattr(result, cat)\n",
    "        print(f\"  {cat.upper()}: [{ffo.sentiment.value}] {ffo.summary[:60]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "from kopi_sentiment.scraper.reddit import RedditScraper\n",
    "\n",
    "# Topics to search for diverse FFO content\n",
    "topics = [\n",
    "    \"HDB\",\n",
    "    \"BTO\",\n",
    "    \"property\",\n",
    "    \"prices\",\n",
    "    \"economy\",\n",
    "    \"inflation\",\n",
    "]\n",
    "\n",
    "scraper = RedditScraper()\n",
    "all_comments = []\n",
    "\n",
    "for topic in topics:\n",
    "    print(f\"Searching for: {topic}\")\n",
    "    posts = scraper.search_posts_with_content(topic, limit=7, time_filter='year',delay=2.0)\n",
    "    \n",
    "    for post in posts:\n",
    "        for comment in post.comments:\n",
    "            all_comments.append({\n",
    "                \"text\": comment.text,\n",
    "                \"score\": comment.score,\n",
    "                \"post_title\": post.title,\n",
    "                \"post_id\": post.id,\n",
    "                \"topic\": topic,\n",
    "            })\n",
    "    \n",
    "    print(f\"  Found {len(posts)} posts, total comments: {len(all_comments)}\")\n",
    "    time.sleep(2)  # Rate limiting between topics\n",
    "\n",
    "print(f\"\\nTotal comments collected: {len(all_comments)}\")\n",
    "\n",
    "# Save to CSV\n",
    "# Get project root (parent of research/)\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "data_path = os.path.join(project_root, \"data\", \"comments_raw.csv\")\n",
    "\n",
    "# Ensure data folder exists\n",
    "os.makedirs(os.path.dirname(data_path), exist_ok=True)\n",
    "\n",
    "with open(data_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"text\", \"score\", \"post_title\", \"post_id\", \"topic\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_comments)\n",
    "\n",
    "print(\"Saved to data/comments_raw.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/comments_raw.csv\")\n",
    "print(f\"Total comments: {len(df)}\")\n",
    "print(f\"\\nComments per topic:\")\n",
    "print(df['topic'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from anthropic import Anthropic\n",
    "from kopi_sentiment.config.settings import settings\n",
    "\n",
    "# Load raw comments (start fresh for dual labeling)\n",
    "df = pd.read_csv(\"../data/comments_raw.csv\")\n",
    "# Filter for comments with score > 30\n",
    "df = df[df['score'] > 30].reset_index(drop=True)\n",
    "print(f\"Comments with score > 30: {len(df)}\")\n",
    "\n",
    "client = Anthropic(api_key=settings.anthropic_api_key)\n",
    "\n",
    "BATCH_LABELING_PROMPT = \"\"\"You are an expert sociologist analyzing Reddit comments. Your task is to classify each comment based on its underlying *motivation and intensity*, not just its literal surface-level meaning. A comment might use neutral data to express a deep-seated emotion.\n",
    "\n",
    "Classify each Reddit comment into exactly ONE category and ONE intensity level.\n",
    "\n",
    "Categories:\n",
    "- fear: Worries, anxieties, or negative predictions about the FUTURE. Often expressed through projecting current negative trends forward.\n",
    "- frustration: Complaints about a CURRENT situation. Expresses anger, annoyance, or helplessness about things not working as they should NOW.\n",
    "- goal: A specific, actionable plan the person is trying to achieve.\n",
    "- aspiration: A general hope or dream for an ideal future state.\n",
    "- none: Neutral comments. This includes simple jokes, questions for information, or comments that do not express any underlying emotional stance.\n",
    "\n",
    "**Important:** If a comment uses facts, data, or projections to highlight a problem (e.g., rising costs, stagnating wages, alarming future numbers), classify it as 'fear' or 'frustration', NOT 'none'.\n",
    "\n",
    "Intensity (how STRONGLY the FFO emotion is expressed):\n",
    "- mild: Slight mention, passing concern, casual reference\n",
    "- moderate: Clear expression, noticeable feeling, definite stance\n",
    "- strong: Intense, emphatic, passionate expression (e.g., caps, exclamation marks, strong language)\n",
    "\n",
    "Note: Intensity is about HOW STRONGLY the emotion is felt, not whether it's positive or negative.\n",
    "- A mild fear is a slight worry\n",
    "- A strong fear is deep anxiety or panic\n",
    "- A mild frustration is minor annoyance\n",
    "- A strong frustration is outrage or anger\n",
    "\n",
    "Comments:\n",
    "{comments_block}\n",
    "\n",
    "Respond in JSON array format:\n",
    "[\n",
    "  {{\"id\": 0, \"category\": \"...\", \"intensity\": \"...\"}},\n",
    "  {{\"id\": 1, \"category\": \"...\", \"intensity\": \"...\"}}\n",
    "]\n",
    "\n",
    "ONLY output the JSON array, nothing else.\"\"\"\n",
    "\n",
    "\n",
    "def label_batch(comments: list[tuple[int, str]]) -> list[dict]:\n",
    "    \"\"\"Label a batch of comments, returns list of {id, category, intensity}.\"\"\"\n",
    "    comments_block = \"\\n\".join([f\"[{i}] {text[:500]}\" for i, text in comments])\n",
    "    \n",
    "    try:\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-sonnet-4-20250514\",\n",
    "            max_tokens=1024,\n",
    "            messages=[{\"role\": \"user\", \"content\": BATCH_LABELING_PROMPT.format(comments_block=comments_block)}]\n",
    "        )\n",
    "        raw = response.content[0].text.strip()\n",
    "        # Clean markdown if present\n",
    "        if raw.startswith(\"```\"):\n",
    "            raw = raw.split(\"```\")[1]\n",
    "            if raw.startswith(\"json\"):\n",
    "                raw = raw[4:]\n",
    "        return json.loads(raw)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return [{\"id\": i, \"category\": \"error\", \"intensity\": \"error\"} for i, _ in comments]\n",
    "\n",
    "# Initialize columns\n",
    "df['category'] = None\n",
    "df['intensity'] = None\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "total_batches = (len(df) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "for batch_num in range(total_batches):\n",
    "    start_idx = batch_num * BATCH_SIZE\n",
    "    end_idx = min(start_idx + BATCH_SIZE, len(df))\n",
    "    \n",
    "    # Build batch: list of (local_id, text)\n",
    "    batch = [(i - start_idx, df.iloc[i]['text']) for i in range(start_idx, end_idx)]\n",
    "    \n",
    "    # Label batch\n",
    "    results = label_batch(batch)\n",
    "    \n",
    "    # Apply results\n",
    "    for result in results:\n",
    "        row_idx = start_idx + result['id']\n",
    "        if row_idx < len(df):\n",
    "            df.at[row_idx, 'category'] = result.get('category', 'error')\n",
    "            df.at[row_idx, 'intensity'] = result.get('intensity', 'error')\n",
    "    \n",
    "    # Progress & checkpoint\n",
    "    if (batch_num + 1) % 5 == 0:\n",
    "        df.to_csv(\"../data/comments_labeled.csv\", index=False)\n",
    "        print(f\"Batch {batch_num + 1}/{total_batches} done, saved checkpoint\")\n",
    "    \n",
    "    time.sleep(0.5)  # Rate limiting\n",
    "\n",
    "# Final save\n",
    "df.to_csv(\"../data/comments_labeled.csv\", index=False)\n",
    "print(f\"\\nLabeling complete!\")\n",
    "print(f\"\\nCategory distribution:\\n{df['category'].value_counts()}\")\n",
    "print(f\"\\nIntensity distribution:\\n{df['intensity'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Create validation sample for manual labeling (with intensity)\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "\n",
    "# Load the labeled data\n",
    "df = pd.read_csv(\"../data/comments_labeled.csv\")\n",
    "print(f\"Total comments: {len(df)}\")\n",
    "\n",
    "# Filter by score > 30 for all intensities\n",
    "df_filtered = df[df['score'] > 20]\n",
    "df_filtered = df_filtered.drop_duplicates(subset=\"text\")\n",
    "print(f\"Comments with score > 20: {len(df_filtered)}\")\n",
    "\n",
    "# Sample 100 (or all if less than 100)\n",
    "n_sample = min(100, len(df_filtered))\n",
    "validation_sample = df_filtered.sample(n=n_sample, random_state=42)\n",
    "\n",
    "# Add empty columns for human labels\n",
    "validation_sample = validation_sample.copy()\n",
    "validation_sample['human_category'] = ''\n",
    "validation_sample['human_intensity'] = ''\n",
    "\n",
    "# Reorder columns for easier labeling\n",
    "cols = ['text', 'human_category', 'human_intensity', 'category', 'intensity', 'score', 'post_title', 'topic']\n",
    "validation_sample = validation_sample[cols]\n",
    "\n",
    "# Save to CSV\n",
    "validation_sample.to_csv(\"../data/validation_sample.csv\", index=False)\n",
    "\n",
    "print(f\"\\nSaved {n_sample} comments to data/validation_sample.csv\")\n",
    "print(\"\\nCategories: fear, frustration, goal, aspiration, none\")\n",
    "print(\"Intensities: mild, moderate, strong\")\n",
    "print(f\"\\nLLM category distribution in sample:\\n{validation_sample['category'].value_counts()}\")\n",
    "print(f\"\\nLLM intensity distribution in sample:\\n{validation_sample['intensity'].value_counts()}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Module 4: Traditional ML Baseline (Enhanced)\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Load labeled data\n",
    "df = pd.read_csv(\"../data/comments_labeled.csv\")\n",
    "df = df.drop_duplicates(subset=\"text\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"\\nCategory distribution:\\n{df['category'].value_counts()}\")\n",
    "print(f\"\\nIntensity distribution:\\n{df['intensity'].value_counts()}\")\n",
    "\n",
    "# Prepare features\n",
    "X = df['text']\n",
    "\n",
    "# Encode labels (required for MLP)\n",
    "le_cat = LabelEncoder()\n",
    "le_int = LabelEncoder()\n",
    "\n",
    "y_category = le_cat.fit_transform(df['category'])\n",
    "y_intensity = le_int.fit_transform(df['intensity'])\n",
    "\n",
    "print(f\"\\nCategory classes: {le_cat.classes_}\")\n",
    "print(f\"Intensity classes: {le_int.classes_}\")\n",
    "\n",
    "# Train/Test Split (80/20, stratified)\n",
    "X_train, X_test, y_cat_train, y_cat_test, y_int_train, y_int_test = train_test_split(\n",
    "    X, y_category, y_intensity, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_category\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain size: {len(X_train)}, Test size: {len(X_test)}\")\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words='english',\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "print(f\"TF-IDF shape: {X_train_tfidf.shape}\")\n",
    "\n",
    "# Define models to compare\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced'),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    \"MLP Neural Net\": MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=500, random_state=42, early_stopping=True)\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# CATEGORY CLASSIFICATION\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CATEGORY CLASSIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cat_results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_tfidf, y_cat_train)\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    f1 = f1_score(y_cat_test, y_pred, average='macro')\n",
    "    cat_results[name] = {'model': model, 'f1': f1}\n",
    "    print(f\"\\n{name}: Test F1 Macro: {f1:.3f}\")\n",
    "\n",
    "best_cat_name = max(cat_results, key=lambda x: cat_results[x]['f1'])\n",
    "best_cat_model = cat_results[best_cat_name]['model']\n",
    "print(f\"\\n>>> Best Category Model: {best_cat_name} (F1: {cat_results[best_cat_name]['f1']:.3f})\")\n",
    "\n",
    "print(f\"\\nClassification Report ({best_cat_name}):\")\n",
    "y_pred_best = best_cat_model.predict(X_test_tfidf)\n",
    "print(classification_report(y_cat_test, y_pred_best, target_names=le_cat.classes_))\n",
    "\n",
    "# =============================================================================\n",
    "# INTENSITY CLASSIFICATION\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INTENSITY CLASSIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "models_int = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced'),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    \"MLP Neural Net\": MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=500, random_state=42, early_stopping=True)\n",
    "}\n",
    "\n",
    "int_results = {}\n",
    "for name, model in models_int.items():\n",
    "    model.fit(X_train_tfidf, y_int_train)\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    f1 = f1_score(y_int_test, y_pred, average='macro')\n",
    "    int_results[name] = {'model': model, 'f1': f1}\n",
    "    print(f\"\\n{name}: Test F1 Macro: {f1:.3f}\")\n",
    "\n",
    "best_int_name = max(int_results, key=lambda x: int_results[x]['f1'])\n",
    "best_int_model = int_results[best_int_name]['model']\n",
    "print(f\"\\n>>> Best Intensity Model: {best_int_name} (F1: {int_results[best_int_name]['f1']:.3f})\")\n",
    "\n",
    "print(f\"\\nClassification Report ({best_int_name}):\")\n",
    "y_pred_best = best_int_model.predict(X_test_tfidf)\n",
    "print(classification_report(y_int_test, y_pred_best, target_names=le_int.classes_))\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE BEST MODELS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "# Retrain best models on FULL dataset\n",
    "X_full_tfidf = tfidf.fit_transform(X)\n",
    "y_cat_full = le_cat.fit_transform(df['category'])\n",
    "y_int_full = le_int.fit_transform(df['intensity'])\n",
    "\n",
    "from sklearn.base import clone\n",
    "best_cat_final = clone(cat_results[best_cat_name]['model'])\n",
    "best_cat_final.fit(X_full_tfidf, y_cat_full)\n",
    "\n",
    "best_int_final = clone(int_results[best_int_name]['model'])\n",
    "best_int_final.fit(X_full_tfidf, y_int_full)\n",
    "\n",
    "# Save\n",
    "joblib.dump(tfidf, \"../models/tfidf_vectorizer.joblib\")\n",
    "joblib.dump(best_cat_final, \"../models/best_category_model.joblib\")\n",
    "joblib.dump(best_int_final, \"../models/best_intensity_model.joblib\")\n",
    "joblib.dump(le_cat, \"../models/label_encoder_category.joblib\")\n",
    "joblib.dump(le_int, \"../models/label_encoder_intensity.joblib\")\n",
    "\n",
    "metadata = {\n",
    "    'best_category_model': best_cat_name,\n",
    "    'best_category_f1': cat_results[best_cat_name]['f1'],\n",
    "    'best_intensity_model': best_int_name,\n",
    "    'best_intensity_f1': int_results[best_int_name]['f1'],\n",
    "    'train_size': len(X_train),\n",
    "    'test_size': len(X_test),\n",
    "    'total_samples': len(df),\n",
    "    'category_classes': list(le_cat.classes_),\n",
    "    'intensity_classes': list(le_int.classes_)\n",
    "}\n",
    "joblib.dump(metadata, \"../models/metadata.joblib\")\n",
    "\n",
    "print(f\"Saved: tfidf_vectorizer.joblib, best_category_model.joblib, best_intensity_model.joblib\")\n",
    "print(f\"Saved: label_encoder_category.joblib, label_encoder_intensity.joblib, metadata.joblib\")\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Model':<25} {'Category F1':<15} {'Intensity F1':<15}\")\n",
    "print(\"-\"*55)\n",
    "for name in models.keys():\n",
    "    cat_f1 = cat_results[name]['f1']\n",
    "    int_f1 = int_results[name]['f1']\n",
    "    print(f\"{name:<25} {cat_f1:<15.3f} {int_f1:<15.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VIEW PREDICTIONS FROM EACH MODEL\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "\n",
    "# Create a dataframe with test samples and predictions\n",
    "test_df = pd.DataFrame({\n",
    "    'text': X_test.values,\n",
    "    'true_category': le_cat.inverse_transform(y_cat_test),\n",
    "    'true_intensity': le_int.inverse_transform(y_int_test)\n",
    "})\n",
    "\n",
    "# Add predictions from each model (Category)\n",
    "for name, result in cat_results.items():\n",
    "    preds = result['model'].predict(X_test_tfidf)\n",
    "    test_df[f'cat_{name.lower().replace(\" \", \"_\")}'] = le_cat.inverse_transform(preds)\n",
    "\n",
    "# Add predictions from each model (Intensity)\n",
    "for name, result in int_results.items():\n",
    "    preds = result['model'].predict(X_test_tfidf)\n",
    "    test_df[f'int_{name.lower().replace(\" \", \"_\")}'] = le_int.inverse_transform(preds)\n",
    "\n",
    "# Show first 20 samples\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "print(\"=\"*100)\n",
    "print(\"CATEGORY PREDICTIONS (first 20 samples)\")\n",
    "print(\"=\"*100)\n",
    "cat_cols = ['text', 'true_category', 'cat_logistic_regression', 'cat_random_forest', 'cat_gradient_boosting', 'cat_mlp_neural_net']\n",
    "print(test_df[cat_cols].head(20).to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"INTENSITY PREDICTIONS (first 20 samples)\")\n",
    "print(\"=\"*100)\n",
    "int_cols = ['text', 'true_intensity', 'int_logistic_regression', 'int_random_forest', 'int_gradient_boosting', 'int_mlp_neural_net']\n",
    "print(test_df[int_cols].head(20).to_string())\n",
    "\n",
    "# Show some misclassified examples\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"MISCLASSIFIED EXAMPLES (Category - Logistic Regression)\")\n",
    "print(\"=\"*100)\n",
    "misclassified = test_df[test_df['true_category'] != test_df['cat_logistic_regression']]\n",
    "print(f\"Total misclassified: {len(misclassified)} / {len(test_df)}\")\n",
    "print(misclassified[['text', 'true_category', 'cat_logistic_regression']].head(10).to_string())\n",
    "\n",
    "# Save full predictions to CSV for review\n",
    "test_df.to_csv(\"../data/model_predictions.csv\", index=False)\n",
    "print(\"\\nFull predictions saved to data/model_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from kopi_sentiment.pipeline.weekly import WeeklyPipeline\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "pipeline = WeeklyPipeline(\n",
    "    subreddits=['singaporeraw'],\n",
    "    posts_per_subreddit=5,\n",
    "    llm_provider='openai',\n",
    "    storage_path='data/weekly'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Week: 2026-W03\n",
      "Posts analyzed: 5\n",
      "Comments analyzed: 79\n",
      "\n",
      "Fears: Singaporeans are increasingly expressing concerns about potential scams and the overall trajectory of societal progress. Many fear that the current path could lead to significant regression in the future.\n",
      "Frustrations: The frustrations surrounding public spaces are intensifying, particularly around the activities of youth in shared areas like MRT stations, which many feel obstruct and endanger pedestrians. People are vocal about feeling overwhelmed by trivial issues showcased in the media while facing larger, unresolved community problems.\n",
      "Goals: There is a collective aspiration for more effective governance and thoughtful urban planning that prioritizes community needs and reduces pedestrian congestion. Some voices are calling for action against complacency in political leadership to better shape Singapore's future.\n",
      "Aspirations: A growing desire for a vibrant cultural landscape that embraces dynamic arts and self-expression is evident among Singaporeans. Many believe that fostering such creative environments will enrich the nation's identity and cultural evolution.\n",
      "\n",
      "Trending topics: ['MRT Safety Concerns', 'Local Media Reporting', 'Community Issues with Neighbors', 'Urban Art and Culture Development', 'Scam Awareness']\n"
     ]
    }
   ],
   "source": [
    "report = pipeline.run(week_id=None)\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Week: {report.week_id}\")\n",
    "print(f\"Posts analyzed: {report.metadata.total_posts_analyzed}\")\n",
    "print(f\"Comments analyzed: {report.metadata.total_comments_analyzed}\")\n",
    "print(f\"\\nFears: {report.overall_sentiment.fears.summary}\")\n",
    "print(f\"Frustrations: {report.overall_sentiment.frustrations.summary}\")\n",
    "print(f\"Goals: {report.overall_sentiment.goals.summary}\")\n",
    "print(f\"Aspirations: {report.overall_sentiment.aspirations.summary}\")\n",
    "print(f\"\\nTrending topics: {[t.topic for t in report.trending_topics]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kopi_sentiment.pipeline.weekly import WeeklyPipeline\n",
    "\n",
    "pipeline = WeeklyPipeline(\n",
    "    subreddits=['singapore', 'askSingapore', 'singaporeraw', 'singaporefi'],\n",
    "    posts_per_subreddit=10,\n",
    "    llm_provider='claude',\n",
    "    storage_path='research/data'\n",
    ")\n",
    "\n",
    "report = pipeline.run(week_id='2026-W03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kopi_sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
